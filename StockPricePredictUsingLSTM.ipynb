{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftFLd2VNothj",
        "outputId": "85540855-b808-4a30-f0dd-88a84ae042ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.9/dist-packages (2.11.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (1.3.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (1.22.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (3.5.3)\n",
            "Requirement already satisfied: yahoo_fin in /usr/local/lib/python3.9/dist-packages (0.8.9.1)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.9/dist-packages (0.0.post1)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.19.6)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow) (57.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (15.0.6.1)\n",
            "Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.11.2)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.51.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.31.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.11.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: keras<2.12,>=2.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.11.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow) (23.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (23.3.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.9/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.9/dist-packages (from pandas) (2022.7.1)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (8.4.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (4.39.0)\n",
            "Requirement already satisfied: requests-html in /usr/local/lib/python3.9/dist-packages (from yahoo_fin) (0.10.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from yahoo_fin) (2.25.1)\n",
            "Requirement already satisfied: feedparser in /usr/local/lib/python3.9/dist-packages (from yahoo_fin) (6.0.10)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.16.2)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests->yahoo_fin) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->yahoo_fin) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->yahoo_fin) (1.26.14)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->yahoo_fin) (2.10)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.9/dist-packages (from feedparser->yahoo_fin) (1.0.0)\n",
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.9/dist-packages (from requests-html->yahoo_fin) (0.0.1)\n",
            "Requirement already satisfied: pyppeteer>=0.0.14 in /usr/local/lib/python3.9/dist-packages (from requests-html->yahoo_fin) (1.0.2)\n",
            "Requirement already satisfied: parse in /usr/local/lib/python3.9/dist-packages (from requests-html->yahoo_fin) (1.19.0)\n",
            "Requirement already satisfied: pyquery in /usr/local/lib/python3.9/dist-packages (from requests-html->yahoo_fin) (2.0.0)\n",
            "Requirement already satisfied: w3lib in /usr/local/lib/python3.9/dist-packages (from requests-html->yahoo_fin) (2.1.1)\n",
            "Requirement already satisfied: fake-useragent in /usr/local/lib/python3.9/dist-packages (from requests-html->yahoo_fin) (1.1.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (5.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (6.0.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.42.1 in /usr/local/lib/python3.9/dist-packages (from pyppeteer>=0.0.14->requests-html->yahoo_fin) (4.65.0)\n",
            "Requirement already satisfied: websockets<11.0,>=10.0 in /usr/local/lib/python3.9/dist-packages (from pyppeteer>=0.0.14->requests-html->yahoo_fin) (10.4)\n",
            "Requirement already satisfied: pyee<9.0.0,>=8.1.0 in /usr/local/lib/python3.9/dist-packages (from pyppeteer>=0.0.14->requests-html->yahoo_fin) (8.2.2)\n",
            "Requirement already satisfied: appdirs<2.0.0,>=1.4.3 in /usr/local/lib/python3.9/dist-packages (from pyppeteer>=0.0.14->requests-html->yahoo_fin) (1.4.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow) (2.1.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.9/dist-packages (from bs4->requests-html->yahoo_fin) (4.6.3)\n",
            "Requirement already satisfied: importlib-resources>=5.0 in /usr/local/lib/python3.9/dist-packages (from fake-useragent->requests-html->yahoo_fin) (5.12.0)\n",
            "Requirement already satisfied: cssselect>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from pyquery->requests-html->yahoo_fin) (1.2.0)\n",
            "Requirement already satisfied: lxml>=2.1 in /usr/local/lib/python3.9/dist-packages (from pyquery->requests-html->yahoo_fin) (4.9.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install tensorflow pandas numpy matplotlib yahoo_fin sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nWYlmmM4pn7D"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from yahoo_fin import stock_info as si\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2J1HL8VqPuf"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "talE4YAiqP4d"
      },
      "outputs": [],
      "source": [
        "np.random.seed(422)\n",
        "tf.random.set_seed(422)\n",
        "random.seed(422)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHfsoLfbqoFB"
      },
      "outputs": [],
      "source": [
        "def shuffle_in_unison(a, b):\n",
        "    # shuffle two arrays in the same way\n",
        "    state = np.random.get_state()\n",
        "    np.random.shuffle(a)\n",
        "    np.random.set_state(state)\n",
        "    np.random.shuffle(b)\n",
        "\n",
        "def load_data(ticker, n_steps=50, scale=True, shuffle=True, lookup_step=1, split_by_date=True,\n",
        "                test_size=0.2, feature_columns=['adjclose', 'volume', 'open', 'high', 'low']):\n",
        "    \"\"\"\n",
        "    Loads data from Yahoo Finance source, as well as scaling, shuffling, normalizing and splitting.\n",
        "    Params:\n",
        "        ticker (str/pd.DataFrame): the ticker you want to load, examples include AAPL, TESL, etc.\n",
        "        n_steps (int): the historical sequence length (i.e window size) used to predict, default is 50\n",
        "        scale (bool): whether to scale prices from 0 to 1, default is True\n",
        "        shuffle (bool): whether to shuffle the dataset (both training & testing), default is True\n",
        "        lookup_step (int): the future lookup step to predict, default is 1 (e.g next day)\n",
        "        split_by_date (bool): whether we split the dataset into training/testing by date, setting it \n",
        "            to False will split datasets in a random way\n",
        "        test_size (float): ratio for test data, default is 0.2 (20% testing data)\n",
        "        feature_columns (list): the list of features to use to feed into the model, default is everything grabbed from yahoo_fin\n",
        "    \"\"\"\n",
        "    # see if ticker is already a loaded stock from yahoo finance\n",
        "    if isinstance(ticker, str):\n",
        "        # load it from yahoo_fin library\n",
        "        df = si.get_data(ticker)\n",
        "    elif isinstance(ticker, pd.DataFrame):\n",
        "        # already loaded, use it directly\n",
        "        df = ticker\n",
        "    else:\n",
        "        raise TypeError(\"ticker can be either a str or a `pd.DataFrame` instances\")\n",
        "    # this will contain all the elements we want to return from this function\n",
        "    result = {}\n",
        "    # we will also return the original dataframe itself\n",
        "    result['df'] = df.copy()\n",
        "    # make sure that the passed feature_columns exist in the dataframe\n",
        "    for col in feature_columns:\n",
        "        assert col in df.columns, f\"'{col}' does not exist in the dataframe.\"\n",
        "    # add date as a column\n",
        "    if \"date\" not in df.columns:\n",
        "        df[\"date\"] = df.index\n",
        "    if scale:\n",
        "        column_scaler = {}\n",
        "        # scale the data (prices) from 0 to 1\n",
        "        for column in feature_columns:\n",
        "            scaler = preprocessing.MinMaxScaler()\n",
        "            df[column] = scaler.fit_transform(np.expand_dims(df[column].values, axis=1))\n",
        "            column_scaler[column] = scaler\n",
        "        # add the MinMaxScaler instances to the result returned\n",
        "        result[\"column_scaler\"] = column_scaler\n",
        "    # add the target column (label) by shifting by `lookup_step`\n",
        "    df['future'] = df['adjclose'].shift(-lookup_step)\n",
        "    # last `lookup_step` columns contains NaN in future column\n",
        "    # get them before droping NaNs\n",
        "    last_sequence = np.array(df[feature_columns].tail(lookup_step))\n",
        "    # drop NaNs\n",
        "    df.dropna(inplace=True)\n",
        "    sequence_data = []\n",
        "    sequences = deque(maxlen=n_steps)\n",
        "    for entry, target in zip(df[feature_columns + [\"date\"]].values, df['future'].values):\n",
        "        sequences.append(entry)\n",
        "        if len(sequences) == n_steps:\n",
        "            sequence_data.append([np.array(sequences), target])\n",
        "    # get the last sequence by appending the last `n_step` sequence with `lookup_step` sequence\n",
        "    # for instance, if n_steps=50 and lookup_step=10, last_sequence should be of 60 (that is 50+10) length\n",
        "    # this last_sequence will be used to predict future stock prices that are not available in the dataset\n",
        "    last_sequence = list([s[:len(feature_columns)] for s in sequences]) + list(last_sequence)\n",
        "    last_sequence = np.array(last_sequence).astype(np.float32)\n",
        "    # add to result\n",
        "    result['last_sequence'] = last_sequence\n",
        "    # construct the X's and y's\n",
        "    X, y = [], []\n",
        "    for seq, target in sequence_data:\n",
        "        X.append(seq)\n",
        "        y.append(target)\n",
        "    # convert to numpy arrays\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "    if split_by_date:\n",
        "        # split the dataset into training & testing sets by date (not randomly splitting)\n",
        "        train_samples = int((1 - test_size) * len(X))\n",
        "        result[\"X_train\"] = X[:train_samples]\n",
        "        result[\"y_train\"] = y[:train_samples]\n",
        "        result[\"X_test\"]  = X[train_samples:]\n",
        "        result[\"y_test\"]  = y[train_samples:]\n",
        "        if shuffle:\n",
        "            # shuffle the datasets for training (if shuffle parameter is set)\n",
        "            shuffle_in_unison(result[\"X_train\"], result[\"y_train\"])\n",
        "            shuffle_in_unison(result[\"X_test\"], result[\"y_test\"])\n",
        "    else:    \n",
        "        # split the dataset randomly\n",
        "        result[\"X_train\"], result[\"X_test\"], result[\"y_train\"], result[\"y_test\"] = train_test_split(X, y, \n",
        "                                                                                test_size=test_size, shuffle=shuffle)\n",
        "    # get the list of test set dates\n",
        "    dates = result[\"X_test\"][:, -1, -1]\n",
        "    # retrieve test features from the original dataframe\n",
        "    result[\"test_df\"] = result[\"df\"].loc[dates]\n",
        "    # remove duplicated dates in the testing dataframe\n",
        "    result[\"test_df\"] = result[\"test_df\"][~result[\"test_df\"].index.duplicated(keep='first')]\n",
        "    # remove dates from the training/testing sets & convert to float32\n",
        "    result[\"X_train\"] = result[\"X_train\"][:, :, :len(feature_columns)].astype(np.float32)\n",
        "    result[\"X_test\"] = result[\"X_test\"][:, :, :len(feature_columns)].astype(np.float32)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-C2jWipdq3Rl"
      },
      "outputs": [],
      "source": [
        "def create_model(sequence_length, n_features, units=256, cell=LSTM, n_layers=2, dropout=0.3,\n",
        "                loss=\"mean_absolute_error\", optimizer=\"rmsprop\", bidirectional=False):\n",
        "    model = Sequential()\n",
        "    for i in range(n_layers):\n",
        "        if i == 0:\n",
        "            # first layer\n",
        "            if bidirectional:\n",
        "                model.add(Bidirectional(cell(units, return_sequences=True), batch_input_shape=(None, sequence_length, n_features)))\n",
        "            else:\n",
        "                model.add(cell(units, return_sequences=True, batch_input_shape=(None, sequence_length, n_features)))\n",
        "        elif i == n_layers - 1:\n",
        "            # last layer\n",
        "            if bidirectional:\n",
        "                model.add(Bidirectional(cell(units, return_sequences=False)))\n",
        "            else:\n",
        "                model.add(cell(units, return_sequences=False))\n",
        "        else:\n",
        "            # hidden layers\n",
        "            if bidirectional:\n",
        "                model.add(Bidirectional(cell(units, return_sequences=True)))\n",
        "            else:\n",
        "                model.add(cell(units, return_sequences=True))\n",
        "        # add dropout after each layer\n",
        "        model.add(Dropout(dropout))\n",
        "    model.add(Dense(1, activation=\"linear\"))\n",
        "    model.compile(loss=loss, metrics=[\"mean_absolute_error\"], optimizer=optimizer)\n",
        "    return model\n",
        "    model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hnctld2sq4O_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "from tensorflow.keras.layers import LSTM\n",
        "\n",
        "# Window size or the sequence length\n",
        "N_STEPS = 50\n",
        "# Lookup step, 1 is the next day\n",
        "LOOKUP_STEP = 15\n",
        "# whether to scale feature columns & output price as well\n",
        "SCALE = True\n",
        "scale_str = f\"sc-{int(SCALE)}\"\n",
        "# whether to shuffle the dataset\n",
        "SHUFFLE = True\n",
        "shuffle_str = f\"sh-{int(SHUFFLE)}\"\n",
        "# whether to split the training/testing set by date\n",
        "SPLIT_BY_DATE = False\n",
        "split_by_date_str = f\"sbd-{int(SPLIT_BY_DATE)}\"\n",
        "# test ratio size, 0.2 is 20%\n",
        "TEST_SIZE = 0.2\n",
        "# features to use\n",
        "FEATURE_COLUMNS = [\"adjclose\", \"volume\", \"open\", \"high\", \"low\"]\n",
        "# date now\n",
        "date_now = time.strftime(\"%Y-%m-%d\")\n",
        "### model parameters\n",
        "N_LAYERS = 2\n",
        "# LSTM cell\n",
        "CELL = LSTM\n",
        "# 256 LSTM neurons\n",
        "UNITS = 256\n",
        "# 40% dropout\n",
        "DROPOUT = 0.4\n",
        "# whether to use bidirectional RNNs\n",
        "BIDIRECTIONAL = False\n",
        "### training parameters\n",
        "# mean absolute error loss\n",
        "# LOSS = \"mae\"\n",
        "# huber loss\n",
        "LOSS = \"huber_loss\"\n",
        "OPTIMIZER = \"adam\"\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 500\n",
        "# Apple stock market\n",
        "ticker = \"AAPL\"\n",
        "ticker_data_filename = os.path.join(\"data\", f\"{ticker}_{date_now}.csv\")\n",
        "# model name to save, making it as unique as possible based on parameters\n",
        "model_name = f\"{date_now}_{ticker}-{shuffle_str}-{scale_str}-{split_by_date_str}-\\\n",
        "{LOSS}-{OPTIMIZER}-{CELL.__name__}-seq-{N_STEPS}-step-{LOOKUP_STEP}-layers-{N_LAYERS}-units-{UNITS}\"\n",
        "if BIDIRECTIONAL:\n",
        "    model_name += \"-b\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IUZR06U8q83x"
      },
      "outputs": [],
      "source": [
        "# create these folders if they does not exist\n",
        "if not os.path.isdir(\"results\"):\n",
        "    os.mkdir(\"results\")\n",
        "if not os.path.isdir(\"logs\"):\n",
        "    os.mkdir(\"logs\")\n",
        "if not os.path.isdir(\"data\"):\n",
        "    os.mkdir(\"data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JEProykHq_9t",
        "outputId": "9f9cc2c8-6cc8-4165-88c5-833f230b4c8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "132/133 [============================>.] - ETA: 0s - loss: 7.9100e-04 - mean_absolute_error: 0.0157\n",
            "Epoch 1: val_loss improved from inf to 0.00022, saving model to results/2023-03-10_AAPL-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
            "133/133 [==============================] - 12s 20ms/step - loss: 7.8947e-04 - mean_absolute_error: 0.0157 - val_loss: 2.1646e-04 - val_mean_absolute_error: 0.0096\n",
            "Epoch 2/500\n",
            "130/133 [============================>.] - ETA: 0s - loss: 4.0089e-04 - mean_absolute_error: 0.0125\n",
            "Epoch 2: val_loss did not improve from 0.00022\n",
            "133/133 [==============================] - 2s 12ms/step - loss: 4.0021e-04 - mean_absolute_error: 0.0126 - val_loss: 3.4087e-04 - val_mean_absolute_error: 0.0117\n",
            "Epoch 3/500\n",
            "131/133 [============================>.] - ETA: 0s - loss: 4.3234e-04 - mean_absolute_error: 0.0134\n",
            "Epoch 3: val_loss did not improve from 0.00022\n",
            "133/133 [==============================] - 2s 12ms/step - loss: 4.3370e-04 - mean_absolute_error: 0.0134 - val_loss: 4.4967e-04 - val_mean_absolute_error: 0.0119\n",
            "Epoch 4/500\n",
            "130/133 [============================>.] - ETA: 0s - loss: 4.1284e-04 - mean_absolute_error: 0.0135\n",
            "Epoch 4: val_loss did not improve from 0.00022\n",
            "133/133 [==============================] - 2s 12ms/step - loss: 4.1037e-04 - mean_absolute_error: 0.0134 - val_loss: 5.9794e-04 - val_mean_absolute_error: 0.0136\n",
            "Epoch 5/500\n",
            "131/133 [============================>.] - ETA: 0s - loss: 3.6104e-04 - mean_absolute_error: 0.0122\n",
            "Epoch 5: val_loss improved from 0.00022 to 0.00015, saving model to results/2023-03-10_AAPL-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 3.5994e-04 - mean_absolute_error: 0.0122 - val_loss: 1.5402e-04 - val_mean_absolute_error: 0.0070\n",
            "Epoch 6/500\n",
            "132/133 [============================>.] - ETA: 0s - loss: 3.5005e-04 - mean_absolute_error: 0.0116\n",
            "Epoch 6: val_loss did not improve from 0.00015\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 3.4998e-04 - mean_absolute_error: 0.0116 - val_loss: 7.1685e-04 - val_mean_absolute_error: 0.0180\n",
            "Epoch 7/500\n",
            "133/133 [==============================] - ETA: 0s - loss: 3.6180e-04 - mean_absolute_error: 0.0127\n",
            "Epoch 7: val_loss did not improve from 0.00015\n",
            "133/133 [==============================] - 2s 14ms/step - loss: 3.6180e-04 - mean_absolute_error: 0.0127 - val_loss: 3.4118e-04 - val_mean_absolute_error: 0.0086\n",
            "Epoch 8/500\n",
            "132/133 [============================>.] - ETA: 0s - loss: 3.5739e-04 - mean_absolute_error: 0.0125\n",
            "Epoch 8: val_loss improved from 0.00015 to 0.00015, saving model to results/2023-03-10_AAPL-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 3.5700e-04 - mean_absolute_error: 0.0125 - val_loss: 1.5220e-04 - val_mean_absolute_error: 0.0073\n",
            "Epoch 9/500\n",
            "131/133 [============================>.] - ETA: 0s - loss: 3.0714e-04 - mean_absolute_error: 0.0116\n",
            "Epoch 9: val_loss improved from 0.00015 to 0.00014, saving model to results/2023-03-10_AAPL-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 3.0724e-04 - mean_absolute_error: 0.0116 - val_loss: 1.4168e-04 - val_mean_absolute_error: 0.0065\n",
            "Epoch 10/500\n",
            "129/133 [============================>.] - ETA: 0s - loss: 2.9625e-04 - mean_absolute_error: 0.0112\n",
            "Epoch 10: val_loss did not improve from 0.00014\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 2.9886e-04 - mean_absolute_error: 0.0112 - val_loss: 1.5325e-04 - val_mean_absolute_error: 0.0072\n",
            "Epoch 11/500\n",
            "133/133 [==============================] - ETA: 0s - loss: 2.8279e-04 - mean_absolute_error: 0.0112\n",
            "Epoch 11: val_loss did not improve from 0.00014\n",
            "133/133 [==============================] - 2s 14ms/step - loss: 2.8279e-04 - mean_absolute_error: 0.0112 - val_loss: 4.5964e-04 - val_mean_absolute_error: 0.0115\n",
            "Epoch 12/500\n",
            "129/133 [============================>.] - ETA: 0s - loss: 3.4008e-04 - mean_absolute_error: 0.0123\n",
            "Epoch 12: val_loss did not improve from 0.00014\n",
            "133/133 [==============================] - 2s 12ms/step - loss: 3.3749e-04 - mean_absolute_error: 0.0122 - val_loss: 2.4423e-04 - val_mean_absolute_error: 0.0097\n",
            "Epoch 13/500\n",
            "130/133 [============================>.] - ETA: 0s - loss: 2.7570e-04 - mean_absolute_error: 0.0108\n",
            "Epoch 13: val_loss did not improve from 0.00014\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 2.7506e-04 - mean_absolute_error: 0.0108 - val_loss: 1.5316e-04 - val_mean_absolute_error: 0.0060\n",
            "Epoch 14/500\n",
            "131/133 [============================>.] - ETA: 0s - loss: 2.5103e-04 - mean_absolute_error: 0.0108\n",
            "Epoch 14: val_loss did not improve from 0.00014\n",
            "133/133 [==============================] - 2s 14ms/step - loss: 2.5067e-04 - mean_absolute_error: 0.0108 - val_loss: 1.7819e-04 - val_mean_absolute_error: 0.0076\n",
            "Epoch 15/500\n",
            "131/133 [============================>.] - ETA: 0s - loss: 2.5768e-04 - mean_absolute_error: 0.0108\n",
            "Epoch 15: val_loss improved from 0.00014 to 0.00014, saving model to results/2023-03-10_AAPL-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 2.5677e-04 - mean_absolute_error: 0.0108 - val_loss: 1.3672e-04 - val_mean_absolute_error: 0.0060\n",
            "Epoch 16/500\n",
            "129/133 [============================>.] - ETA: 0s - loss: 2.8462e-04 - mean_absolute_error: 0.0115\n",
            "Epoch 16: val_loss did not improve from 0.00014\n",
            "133/133 [==============================] - 2s 14ms/step - loss: 2.8236e-04 - mean_absolute_error: 0.0115 - val_loss: 1.5348e-04 - val_mean_absolute_error: 0.0070\n",
            "Epoch 17/500\n",
            "131/133 [============================>.] - ETA: 0s - loss: 2.7175e-04 - mean_absolute_error: 0.0108\n",
            "Epoch 17: val_loss improved from 0.00014 to 0.00014, saving model to results/2023-03-10_AAPL-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
            "133/133 [==============================] - 2s 14ms/step - loss: 2.7104e-04 - mean_absolute_error: 0.0108 - val_loss: 1.3642e-04 - val_mean_absolute_error: 0.0063\n",
            "Epoch 18/500\n",
            "130/133 [============================>.] - ETA: 0s - loss: 2.6440e-04 - mean_absolute_error: 0.0108\n",
            "Epoch 18: val_loss did not improve from 0.00014\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 2.6530e-04 - mean_absolute_error: 0.0108 - val_loss: 2.6351e-04 - val_mean_absolute_error: 0.0116\n",
            "Epoch 19/500\n",
            "129/133 [============================>.] - ETA: 0s - loss: 2.5979e-04 - mean_absolute_error: 0.0111\n",
            "Epoch 19: val_loss did not improve from 0.00014\n",
            "133/133 [==============================] - 2s 14ms/step - loss: 2.5979e-04 - mean_absolute_error: 0.0111 - val_loss: 1.4234e-04 - val_mean_absolute_error: 0.0062\n",
            "Epoch 20/500\n",
            "131/133 [============================>.] - ETA: 0s - loss: 2.6792e-04 - mean_absolute_error: 0.0112\n",
            "Epoch 20: val_loss improved from 0.00014 to 0.00013, saving model to results/2023-03-10_AAPL-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
            "133/133 [==============================] - 2s 14ms/step - loss: 2.6792e-04 - mean_absolute_error: 0.0112 - val_loss: 1.3252e-04 - val_mean_absolute_error: 0.0055\n",
            "Epoch 21/500\n",
            "133/133 [==============================] - ETA: 0s - loss: 2.7411e-04 - mean_absolute_error: 0.0113\n",
            "Epoch 21: val_loss did not improve from 0.00013\n",
            "133/133 [==============================] - 2s 14ms/step - loss: 2.7411e-04 - mean_absolute_error: 0.0113 - val_loss: 1.3579e-04 - val_mean_absolute_error: 0.0071\n",
            "Epoch 22/500\n",
            "132/133 [============================>.] - ETA: 0s - loss: 2.3932e-04 - mean_absolute_error: 0.0105\n",
            "Epoch 22: val_loss did not improve from 0.00013\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 2.4019e-04 - mean_absolute_error: 0.0106 - val_loss: 1.7604e-04 - val_mean_absolute_error: 0.0065\n",
            "Epoch 23/500\n",
            "130/133 [============================>.] - ETA: 0s - loss: 2.7388e-04 - mean_absolute_error: 0.0114\n",
            "Epoch 23: val_loss did not improve from 0.00013\n",
            "133/133 [==============================] - 2s 12ms/step - loss: 2.7263e-04 - mean_absolute_error: 0.0114 - val_loss: 1.7715e-04 - val_mean_absolute_error: 0.0096\n",
            "Epoch 24/500\n",
            "129/133 [============================>.] - ETA: 0s - loss: 2.6973e-04 - mean_absolute_error: 0.0113\n",
            "Epoch 24: val_loss did not improve from 0.00013\n",
            "133/133 [==============================] - 2s 14ms/step - loss: 2.7110e-04 - mean_absolute_error: 0.0114 - val_loss: 1.3950e-04 - val_mean_absolute_error: 0.0082\n",
            "Epoch 25/500\n",
            "131/133 [============================>.] - ETA: 0s - loss: 2.5458e-04 - mean_absolute_error: 0.0112\n",
            "Epoch 25: val_loss did not improve from 0.00013\n",
            "133/133 [==============================] - 2s 12ms/step - loss: 2.5462e-04 - mean_absolute_error: 0.0112 - val_loss: 1.9836e-04 - val_mean_absolute_error: 0.0081\n",
            "Epoch 26/500\n",
            "131/133 [============================>.] - ETA: 0s - loss: 2.4739e-04 - mean_absolute_error: 0.0110\n",
            "Epoch 26: val_loss did not improve from 0.00013\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 2.4631e-04 - mean_absolute_error: 0.0109 - val_loss: 1.8414e-04 - val_mean_absolute_error: 0.0086\n",
            "Epoch 27/500\n",
            "130/133 [============================>.] - ETA: 0s - loss: 2.3025e-04 - mean_absolute_error: 0.0107\n",
            "Epoch 27: val_loss did not improve from 0.00013\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 2.3336e-04 - mean_absolute_error: 0.0107 - val_loss: 1.7331e-04 - val_mean_absolute_error: 0.0077\n",
            "Epoch 28/500\n",
            "132/133 [============================>.] - ETA: 0s - loss: 2.4049e-04 - mean_absolute_error: 0.0107\n",
            "Epoch 28: val_loss did not improve from 0.00013\n",
            "133/133 [==============================] - 2s 14ms/step - loss: 2.4114e-04 - mean_absolute_error: 0.0107 - val_loss: 1.4515e-04 - val_mean_absolute_error: 0.0061\n",
            "Epoch 29/500\n",
            "132/133 [============================>.] - ETA: 0s - loss: 2.5551e-04 - mean_absolute_error: 0.0112\n",
            "Epoch 29: val_loss did not improve from 0.00013\n",
            "133/133 [==============================] - 2s 17ms/step - loss: 2.5657e-04 - mean_absolute_error: 0.0112 - val_loss: 1.3401e-04 - val_mean_absolute_error: 0.0067\n",
            "Epoch 30/500\n",
            "130/133 [============================>.] - ETA: 0s - loss: 2.6302e-04 - mean_absolute_error: 0.0114\n",
            "Epoch 30: val_loss did not improve from 0.00013\n",
            "133/133 [==============================] - 2s 15ms/step - loss: 2.6036e-04 - mean_absolute_error: 0.0113 - val_loss: 1.9243e-04 - val_mean_absolute_error: 0.0088\n",
            "Epoch 31/500\n",
            "131/133 [============================>.] - ETA: 0s - loss: 2.3053e-04 - mean_absolute_error: 0.0109\n",
            "Epoch 31: val_loss improved from 0.00013 to 0.00013, saving model to results/2023-03-10_AAPL-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 2.2999e-04 - mean_absolute_error: 0.0109 - val_loss: 1.3099e-04 - val_mean_absolute_error: 0.0073\n",
            "Epoch 32/500\n",
            "131/133 [============================>.] - ETA: 0s - loss: 2.4870e-04 - mean_absolute_error: 0.0110\n",
            "Epoch 32: val_loss did not improve from 0.00013\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 2.4862e-04 - mean_absolute_error: 0.0110 - val_loss: 1.3563e-04 - val_mean_absolute_error: 0.0065\n",
            "Epoch 33/500\n",
            "129/133 [============================>.] - ETA: 0s - loss: 2.2007e-04 - mean_absolute_error: 0.0105\n",
            "Epoch 33: val_loss did not improve from 0.00013\n",
            "133/133 [==============================] - 2s 12ms/step - loss: 2.2108e-04 - mean_absolute_error: 0.0106 - val_loss: 1.6729e-04 - val_mean_absolute_error: 0.0062\n",
            "Epoch 34/500\n",
            "129/133 [============================>.] - ETA: 0s - loss: 2.5676e-04 - mean_absolute_error: 0.0112\n",
            "Epoch 34: val_loss did not improve from 0.00013\n",
            "133/133 [==============================] - 2s 14ms/step - loss: 2.5361e-04 - mean_absolute_error: 0.0112 - val_loss: 2.4654e-04 - val_mean_absolute_error: 0.0106\n",
            "Epoch 35/500\n",
            "133/133 [==============================] - ETA: 0s - loss: 2.3378e-04 - mean_absolute_error: 0.0109\n",
            "Epoch 35: val_loss improved from 0.00013 to 0.00013, saving model to results/2023-03-10_AAPL-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
            "133/133 [==============================] - 2s 15ms/step - loss: 2.3378e-04 - mean_absolute_error: 0.0109 - val_loss: 1.2776e-04 - val_mean_absolute_error: 0.0057\n",
            "Epoch 36/500\n",
            "131/133 [============================>.] - ETA: 0s - loss: 2.5746e-04 - mean_absolute_error: 0.0111\n",
            "Epoch 36: val_loss did not improve from 0.00013\n",
            "133/133 [==============================] - 2s 12ms/step - loss: 2.6176e-04 - mean_absolute_error: 0.0112 - val_loss: 1.3299e-04 - val_mean_absolute_error: 0.0063\n",
            "Epoch 37/500\n",
            "131/133 [============================>.] - ETA: 0s - loss: 2.7033e-04 - mean_absolute_error: 0.0118\n",
            "Epoch 37: val_loss did not improve from 0.00013\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 2.6945e-04 - mean_absolute_error: 0.0118 - val_loss: 1.6135e-04 - val_mean_absolute_error: 0.0067\n",
            "Epoch 38/500\n",
            "131/133 [============================>.] - ETA: 0s - loss: 2.1877e-04 - mean_absolute_error: 0.0107\n",
            "Epoch 38: val_loss did not improve from 0.00013\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 2.1856e-04 - mean_absolute_error: 0.0107 - val_loss: 2.1779e-04 - val_mean_absolute_error: 0.0104\n",
            "Epoch 39/500\n",
            "133/133 [==============================] - ETA: 0s - loss: 2.5804e-04 - mean_absolute_error: 0.0113\n",
            "Epoch 39: val_loss did not improve from 0.00013\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 2.5804e-04 - mean_absolute_error: 0.0113 - val_loss: 1.3932e-04 - val_mean_absolute_error: 0.0079\n",
            "Epoch 40/500\n",
            "131/133 [============================>.] - ETA: 0s - loss: 2.3012e-04 - mean_absolute_error: 0.0109\n",
            "Epoch 40: val_loss did not improve from 0.00013\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 2.3069e-04 - mean_absolute_error: 0.0109 - val_loss: 1.4469e-04 - val_mean_absolute_error: 0.0088\n",
            "Epoch 41/500\n",
            "131/133 [============================>.] - ETA: 0s - loss: 2.3516e-04 - mean_absolute_error: 0.0110\n",
            "Epoch 41: val_loss improved from 0.00013 to 0.00012, saving model to results/2023-03-10_AAPL-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
            "133/133 [==============================] - 2s 14ms/step - loss: 2.3740e-04 - mean_absolute_error: 0.0110 - val_loss: 1.2402e-04 - val_mean_absolute_error: 0.0054\n",
            "Epoch 42/500\n",
            "128/133 [===========================>..] - ETA: 0s - loss: 2.4209e-04 - mean_absolute_error: 0.0108\n",
            "Epoch 42: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 2s 14ms/step - loss: 2.4631e-04 - mean_absolute_error: 0.0108 - val_loss: 2.8530e-04 - val_mean_absolute_error: 0.0110\n",
            "Epoch 43/500\n",
            "130/133 [============================>.] - ETA: 0s - loss: 2.4903e-04 - mean_absolute_error: 0.0112\n",
            "Epoch 43: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 2s 12ms/step - loss: 2.4871e-04 - mean_absolute_error: 0.0112 - val_loss: 1.3333e-04 - val_mean_absolute_error: 0.0056\n",
            "Epoch 44/500\n",
            "130/133 [============================>.] - ETA: 0s - loss: 2.3446e-04 - mean_absolute_error: 0.0106\n",
            "Epoch 44: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 2.3719e-04 - mean_absolute_error: 0.0106 - val_loss: 1.4358e-04 - val_mean_absolute_error: 0.0061\n",
            "Epoch 45/500\n",
            "130/133 [============================>.] - ETA: 0s - loss: 2.3721e-04 - mean_absolute_error: 0.0110\n",
            "Epoch 45: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 2s 18ms/step - loss: 2.3862e-04 - mean_absolute_error: 0.0110 - val_loss: 2.1416e-04 - val_mean_absolute_error: 0.0083\n",
            "Epoch 46/500\n",
            "129/133 [============================>.] - ETA: 0s - loss: 2.5436e-04 - mean_absolute_error: 0.0113\n",
            "Epoch 46: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 2s 16ms/step - loss: 2.5387e-04 - mean_absolute_error: 0.0113 - val_loss: 1.3264e-04 - val_mean_absolute_error: 0.0055\n",
            "Epoch 47/500\n",
            "133/133 [==============================] - ETA: 0s - loss: 2.2976e-04 - mean_absolute_error: 0.0110\n",
            "Epoch 47: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 3s 20ms/step - loss: 2.2976e-04 - mean_absolute_error: 0.0110 - val_loss: 1.8184e-04 - val_mean_absolute_error: 0.0080\n",
            "Epoch 48/500\n",
            "133/133 [==============================] - ETA: 0s - loss: 2.4063e-04 - mean_absolute_error: 0.0114\n",
            "Epoch 48: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 3s 24ms/step - loss: 2.4063e-04 - mean_absolute_error: 0.0114 - val_loss: 1.7487e-04 - val_mean_absolute_error: 0.0085\n",
            "Epoch 49/500\n",
            "131/133 [============================>.] - ETA: 0s - loss: 2.2981e-04 - mean_absolute_error: 0.0109\n",
            "Epoch 49: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 2s 17ms/step - loss: 2.3007e-04 - mean_absolute_error: 0.0108 - val_loss: 1.5272e-04 - val_mean_absolute_error: 0.0088\n",
            "Epoch 50/500\n",
            "130/133 [============================>.] - ETA: 0s - loss: 2.3035e-04 - mean_absolute_error: 0.0108\n",
            "Epoch 50: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 2.3566e-04 - mean_absolute_error: 0.0108 - val_loss: 1.5846e-04 - val_mean_absolute_error: 0.0086\n",
            "Epoch 51/500\n",
            "131/133 [============================>.] - ETA: 0s - loss: 2.5176e-04 - mean_absolute_error: 0.0112\n",
            "Epoch 51: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 2.4993e-04 - mean_absolute_error: 0.0112 - val_loss: 1.3181e-04 - val_mean_absolute_error: 0.0064\n",
            "Epoch 52/500\n",
            "130/133 [============================>.] - ETA: 0s - loss: 2.4244e-04 - mean_absolute_error: 0.0112\n",
            "Epoch 52: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 2s 14ms/step - loss: 2.4405e-04 - mean_absolute_error: 0.0112 - val_loss: 1.2930e-04 - val_mean_absolute_error: 0.0058\n",
            "Epoch 53/500\n",
            "129/133 [============================>.] - ETA: 0s - loss: 2.3984e-04 - mean_absolute_error: 0.0108\n",
            "Epoch 53: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 2.4068e-04 - mean_absolute_error: 0.0109 - val_loss: 1.2996e-04 - val_mean_absolute_error: 0.0060\n",
            "Epoch 54/500\n",
            "130/133 [============================>.] - ETA: 0s - loss: 2.7172e-04 - mean_absolute_error: 0.0118\n",
            "Epoch 54: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 2s 15ms/step - loss: 2.7086e-04 - mean_absolute_error: 0.0118 - val_loss: 1.7421e-04 - val_mean_absolute_error: 0.0076\n",
            "Epoch 55/500\n",
            "129/133 [============================>.] - ETA: 0s - loss: 2.6498e-04 - mean_absolute_error: 0.0117\n",
            "Epoch 55: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 2.6204e-04 - mean_absolute_error: 0.0116 - val_loss: 1.9597e-04 - val_mean_absolute_error: 0.0070\n",
            "Epoch 56/500\n",
            "130/133 [============================>.] - ETA: 0s - loss: 2.3652e-04 - mean_absolute_error: 0.0113\n",
            "Epoch 56: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 2s 14ms/step - loss: 2.3664e-04 - mean_absolute_error: 0.0113 - val_loss: 1.2897e-04 - val_mean_absolute_error: 0.0058\n",
            "Epoch 57/500\n",
            "129/133 [============================>.] - ETA: 0s - loss: 2.4130e-04 - mean_absolute_error: 0.0111\n",
            "Epoch 57: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 2.3951e-04 - mean_absolute_error: 0.0111 - val_loss: 1.2866e-04 - val_mean_absolute_error: 0.0071\n",
            "Epoch 58/500\n",
            "133/133 [==============================] - ETA: 0s - loss: 2.1825e-04 - mean_absolute_error: 0.0108\n",
            "Epoch 58: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 2s 14ms/step - loss: 2.1825e-04 - mean_absolute_error: 0.0108 - val_loss: 1.5594e-04 - val_mean_absolute_error: 0.0061\n",
            "Epoch 59/500\n",
            "133/133 [==============================] - ETA: 0s - loss: 2.2069e-04 - mean_absolute_error: 0.0107\n",
            "Epoch 59: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 2.2069e-04 - mean_absolute_error: 0.0107 - val_loss: 1.8020e-04 - val_mean_absolute_error: 0.0070\n",
            "Epoch 60/500\n",
            "133/133 [==============================] - ETA: 0s - loss: 2.0683e-04 - mean_absolute_error: 0.0103\n",
            "Epoch 60: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 2s 14ms/step - loss: 2.0683e-04 - mean_absolute_error: 0.0103 - val_loss: 1.5618e-04 - val_mean_absolute_error: 0.0063\n",
            "Epoch 61/500\n",
            "131/133 [============================>.] - ETA: 0s - loss: 2.0768e-04 - mean_absolute_error: 0.0102\n",
            "Epoch 61: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 2.0728e-04 - mean_absolute_error: 0.0102 - val_loss: 1.3272e-04 - val_mean_absolute_error: 0.0055\n",
            "Epoch 62/500\n",
            "130/133 [============================>.] - ETA: 0s - loss: 2.2143e-04 - mean_absolute_error: 0.0104\n",
            "Epoch 62: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 2.2334e-04 - mean_absolute_error: 0.0104 - val_loss: 1.5316e-04 - val_mean_absolute_error: 0.0059\n",
            "Epoch 63/500\n",
            "130/133 [============================>.] - ETA: 0s - loss: 2.2364e-04 - mean_absolute_error: 0.0107\n",
            "Epoch 63: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 2.2465e-04 - mean_absolute_error: 0.0107 - val_loss: 1.4168e-04 - val_mean_absolute_error: 0.0067\n",
            "Epoch 64/500\n",
            "133/133 [==============================] - ETA: 0s - loss: 2.4283e-04 - mean_absolute_error: 0.0110\n",
            "Epoch 64: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 2s 14ms/step - loss: 2.4283e-04 - mean_absolute_error: 0.0110 - val_loss: 2.0771e-04 - val_mean_absolute_error: 0.0083\n",
            "Epoch 65/500\n",
            "133/133 [==============================] - ETA: 0s - loss: 2.2288e-04 - mean_absolute_error: 0.0107\n",
            "Epoch 65: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 2.2288e-04 - mean_absolute_error: 0.0107 - val_loss: 1.4094e-04 - val_mean_absolute_error: 0.0059\n",
            "Epoch 66/500\n",
            "131/133 [============================>.] - ETA: 0s - loss: 2.3868e-04 - mean_absolute_error: 0.0111\n",
            "Epoch 66: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 2.3724e-04 - mean_absolute_error: 0.0111 - val_loss: 1.3897e-04 - val_mean_absolute_error: 0.0082\n",
            "Epoch 67/500\n",
            "133/133 [==============================] - ETA: 0s - loss: 2.4367e-04 - mean_absolute_error: 0.0112\n",
            "Epoch 67: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 2s 14ms/step - loss: 2.4367e-04 - mean_absolute_error: 0.0112 - val_loss: 1.3681e-04 - val_mean_absolute_error: 0.0062\n",
            "Epoch 68/500\n",
            "129/133 [============================>.] - ETA: 0s - loss: 2.3556e-04 - mean_absolute_error: 0.0108\n",
            "Epoch 68: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 2s 14ms/step - loss: 2.3679e-04 - mean_absolute_error: 0.0108 - val_loss: 1.3046e-04 - val_mean_absolute_error: 0.0068\n",
            "Epoch 69/500\n",
            "129/133 [============================>.] - ETA: 0s - loss: 2.0697e-04 - mean_absolute_error: 0.0103\n",
            "Epoch 69: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 2.0769e-04 - mean_absolute_error: 0.0103 - val_loss: 2.0798e-04 - val_mean_absolute_error: 0.0085\n",
            "Epoch 70/500\n",
            "133/133 [==============================] - ETA: 0s - loss: 2.5426e-04 - mean_absolute_error: 0.0113\n",
            "Epoch 70: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 2s 14ms/step - loss: 2.5426e-04 - mean_absolute_error: 0.0113 - val_loss: 1.2975e-04 - val_mean_absolute_error: 0.0065\n",
            "Epoch 71/500\n",
            "130/133 [============================>.] - ETA: 0s - loss: 2.2289e-04 - mean_absolute_error: 0.0106\n",
            "Epoch 71: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 2.2204e-04 - mean_absolute_error: 0.0106 - val_loss: 1.2594e-04 - val_mean_absolute_error: 0.0068\n",
            "Epoch 72/500\n",
            "130/133 [============================>.] - ETA: 0s - loss: 2.3966e-04 - mean_absolute_error: 0.0110\n",
            "Epoch 72: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 2s 14ms/step - loss: 2.3864e-04 - mean_absolute_error: 0.0109 - val_loss: 1.3453e-04 - val_mean_absolute_error: 0.0059\n",
            "Epoch 73/500\n",
            "130/133 [============================>.] - ETA: 0s - loss: 2.1612e-04 - mean_absolute_error: 0.0109\n",
            "Epoch 73: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 2.1518e-04 - mean_absolute_error: 0.0108 - val_loss: 1.2478e-04 - val_mean_absolute_error: 0.0058\n",
            "Epoch 74/500\n",
            "131/133 [============================>.] - ETA: 0s - loss: 2.0694e-04 - mean_absolute_error: 0.0103\n",
            "Epoch 74: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 2s 15ms/step - loss: 2.0597e-04 - mean_absolute_error: 0.0103 - val_loss: 1.6928e-04 - val_mean_absolute_error: 0.0085\n",
            "Epoch 75/500\n",
            "131/133 [============================>.] - ETA: 0s - loss: 2.0912e-04 - mean_absolute_error: 0.0101\n",
            "Epoch 75: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 2.1036e-04 - mean_absolute_error: 0.0102 - val_loss: 1.3245e-04 - val_mean_absolute_error: 0.0065\n",
            "Epoch 76/500\n",
            "130/133 [============================>.] - ETA: 0s - loss: 2.3414e-04 - mean_absolute_error: 0.0107\n",
            "Epoch 76: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 2s 14ms/step - loss: 2.3411e-04 - mean_absolute_error: 0.0107 - val_loss: 1.6241e-04 - val_mean_absolute_error: 0.0073\n",
            "Epoch 77/500\n",
            "130/133 [============================>.] - ETA: 0s - loss: 2.5639e-04 - mean_absolute_error: 0.0112\n",
            "Epoch 77: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 2s 14ms/step - loss: 2.5281e-04 - mean_absolute_error: 0.0111 - val_loss: 1.2822e-04 - val_mean_absolute_error: 0.0056\n",
            "Epoch 78/500\n",
            "130/133 [============================>.] - ETA: 0s - loss: 2.3765e-04 - mean_absolute_error: 0.0108\n",
            "Epoch 78: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 2s 14ms/step - loss: 2.3647e-04 - mean_absolute_error: 0.0108 - val_loss: 1.4617e-04 - val_mean_absolute_error: 0.0055\n",
            "Epoch 79/500\n",
            "130/133 [============================>.] - ETA: 0s - loss: 2.2797e-04 - mean_absolute_error: 0.0109\n",
            "Epoch 79: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 2s 14ms/step - loss: 2.2790e-04 - mean_absolute_error: 0.0109 - val_loss: 1.2835e-04 - val_mean_absolute_error: 0.0070\n",
            "Epoch 80/500\n",
            "129/133 [============================>.] - ETA: 0s - loss: 2.4993e-04 - mean_absolute_error: 0.0110\n",
            "Epoch 80: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 2.4906e-04 - mean_absolute_error: 0.0110 - val_loss: 1.3645e-04 - val_mean_absolute_error: 0.0068\n",
            "Epoch 81/500\n",
            "131/133 [============================>.] - ETA: 0s - loss: 2.4445e-04 - mean_absolute_error: 0.0111\n",
            "Epoch 81: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 2s 14ms/step - loss: 2.4288e-04 - mean_absolute_error: 0.0111 - val_loss: 1.2667e-04 - val_mean_absolute_error: 0.0060\n",
            "Epoch 82/500\n",
            "129/133 [============================>.] - ETA: 0s - loss: 2.2847e-04 - mean_absolute_error: 0.0106\n",
            "Epoch 82: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 2s 14ms/step - loss: 2.2896e-04 - mean_absolute_error: 0.0106 - val_loss: 1.2780e-04 - val_mean_absolute_error: 0.0062\n",
            "Epoch 83/500\n",
            "130/133 [============================>.] - ETA: 0s - loss: 2.2121e-04 - mean_absolute_error: 0.0106\n",
            "Epoch 83: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 2s 14ms/step - loss: 2.2401e-04 - mean_absolute_error: 0.0107 - val_loss: 2.1742e-04 - val_mean_absolute_error: 0.0087\n",
            "Epoch 84/500\n",
            "129/133 [============================>.] - ETA: 0s - loss: 2.3256e-04 - mean_absolute_error: 0.0108\n",
            "Epoch 84: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 2.3199e-04 - mean_absolute_error: 0.0107 - val_loss: 1.8698e-04 - val_mean_absolute_error: 0.0071\n",
            "Epoch 85/500\n",
            "131/133 [============================>.] - ETA: 0s - loss: 2.3691e-04 - mean_absolute_error: 0.0109\n",
            "Epoch 85: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 2.3750e-04 - mean_absolute_error: 0.0109 - val_loss: 1.5418e-04 - val_mean_absolute_error: 0.0085\n",
            "Epoch 86/500\n",
            "129/133 [============================>.] - ETA: 0s - loss: 2.3187e-04 - mean_absolute_error: 0.0107\n",
            "Epoch 86: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 2s 14ms/step - loss: 2.3064e-04 - mean_absolute_error: 0.0107 - val_loss: 1.2665e-04 - val_mean_absolute_error: 0.0062\n",
            "Epoch 87/500\n",
            "133/133 [==============================] - ETA: 0s - loss: 2.2858e-04 - mean_absolute_error: 0.0107\n",
            "Epoch 87: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 2.2858e-04 - mean_absolute_error: 0.0107 - val_loss: 1.4388e-04 - val_mean_absolute_error: 0.0059\n",
            "Epoch 88/500\n",
            "132/133 [============================>.] - ETA: 0s - loss: 2.2396e-04 - mean_absolute_error: 0.0106\n",
            "Epoch 88: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 2s 14ms/step - loss: 2.2418e-04 - mean_absolute_error: 0.0106 - val_loss: 1.6463e-04 - val_mean_absolute_error: 0.0064\n",
            "Epoch 89/500\n",
            "129/133 [============================>.] - ETA: 0s - loss: 2.2617e-04 - mean_absolute_error: 0.0105\n",
            "Epoch 89: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 2.2581e-04 - mean_absolute_error: 0.0105 - val_loss: 1.3970e-04 - val_mean_absolute_error: 0.0068\n",
            "Epoch 90/500\n",
            "130/133 [============================>.] - ETA: 0s - loss: 2.2528e-04 - mean_absolute_error: 0.0106\n",
            "Epoch 90: val_loss improved from 0.00012 to 0.00012, saving model to results/2023-03-10_AAPL-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
            "133/133 [==============================] - 2s 14ms/step - loss: 2.2737e-04 - mean_absolute_error: 0.0106 - val_loss: 1.2235e-04 - val_mean_absolute_error: 0.0053\n",
            "Epoch 91/500\n",
            "130/133 [============================>.] - ETA: 0s - loss: 2.0554e-04 - mean_absolute_error: 0.0102\n",
            "Epoch 91: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 2s 14ms/step - loss: 2.0705e-04 - mean_absolute_error: 0.0102 - val_loss: 1.4837e-04 - val_mean_absolute_error: 0.0079\n",
            "Epoch 92/500\n",
            "129/133 [============================>.] - ETA: 0s - loss: 2.2990e-04 - mean_absolute_error: 0.0108\n",
            "Epoch 92: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 2s 14ms/step - loss: 2.3336e-04 - mean_absolute_error: 0.0108 - val_loss: 1.4110e-04 - val_mean_absolute_error: 0.0062\n",
            "Epoch 93/500\n",
            "129/133 [============================>.] - ETA: 0s - loss: 2.2400e-04 - mean_absolute_error: 0.0105\n",
            "Epoch 93: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 2.2562e-04 - mean_absolute_error: 0.0105 - val_loss: 1.4373e-04 - val_mean_absolute_error: 0.0064\n",
            "Epoch 94/500\n",
            "129/133 [============================>.] - ETA: 0s - loss: 2.1522e-04 - mean_absolute_error: 0.0105\n",
            "Epoch 94: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 2s 14ms/step - loss: 2.1446e-04 - mean_absolute_error: 0.0105 - val_loss: 1.4274e-04 - val_mean_absolute_error: 0.0057\n",
            "Epoch 95/500\n",
            "132/133 [============================>.] - ETA: 0s - loss: 2.0629e-04 - mean_absolute_error: 0.0102\n",
            "Epoch 95: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 3s 21ms/step - loss: 2.0587e-04 - mean_absolute_error: 0.0102 - val_loss: 1.2604e-04 - val_mean_absolute_error: 0.0053\n",
            "Epoch 96/500\n",
            "130/133 [============================>.] - ETA: 0s - loss: 2.2496e-04 - mean_absolute_error: 0.0103\n",
            "Epoch 96: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 2.2357e-04 - mean_absolute_error: 0.0103 - val_loss: 1.3819e-04 - val_mean_absolute_error: 0.0073\n",
            "Epoch 97/500\n",
            "130/133 [============================>.] - ETA: 0s - loss: 2.1070e-04 - mean_absolute_error: 0.0102\n",
            "Epoch 97: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 2.1073e-04 - mean_absolute_error: 0.0102 - val_loss: 1.2988e-04 - val_mean_absolute_error: 0.0064\n",
            "Epoch 98/500\n",
            "129/133 [============================>.] - ETA: 0s - loss: 2.3263e-04 - mean_absolute_error: 0.0106\n",
            "Epoch 98: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 2s 14ms/step - loss: 2.3381e-04 - mean_absolute_error: 0.0106 - val_loss: 1.7941e-04 - val_mean_absolute_error: 0.0063\n",
            "Epoch 99/500\n",
            "130/133 [============================>.] - ETA: 0s - loss: 2.1241e-04 - mean_absolute_error: 0.0102\n",
            "Epoch 99: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 2s 14ms/step - loss: 2.1102e-04 - mean_absolute_error: 0.0102 - val_loss: 1.3087e-04 - val_mean_absolute_error: 0.0059\n",
            "Epoch 100/500\n",
            "131/133 [============================>.] - ETA: 0s - loss: 2.2247e-04 - mean_absolute_error: 0.0106\n",
            "Epoch 100: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 2.2190e-04 - mean_absolute_error: 0.0106 - val_loss: 1.2424e-04 - val_mean_absolute_error: 0.0066\n",
            "Epoch 101/500\n",
            "132/133 [============================>.] - ETA: 0s - loss: 2.1551e-04 - mean_absolute_error: 0.0105\n",
            "Epoch 101: val_loss improved from 0.00012 to 0.00012, saving model to results/2023-03-10_AAPL-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
            "133/133 [==============================] - 2s 14ms/step - loss: 2.1524e-04 - mean_absolute_error: 0.0105 - val_loss: 1.2165e-04 - val_mean_absolute_error: 0.0062\n",
            "Epoch 102/500\n",
            "130/133 [============================>.] - ETA: 0s - loss: 2.3142e-04 - mean_absolute_error: 0.0107\n",
            "Epoch 102: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 2s 14ms/step - loss: 2.3498e-04 - mean_absolute_error: 0.0108 - val_loss: 1.2988e-04 - val_mean_absolute_error: 0.0062\n",
            "Epoch 103/500\n",
            "129/133 [============================>.] - ETA: 0s - loss: 2.2423e-04 - mean_absolute_error: 0.0105\n",
            "Epoch 103: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 2.2190e-04 - mean_absolute_error: 0.0105 - val_loss: 1.4034e-04 - val_mean_absolute_error: 0.0058\n",
            "Epoch 104/500\n",
            "129/133 [============================>.] - ETA: 0s - loss: 2.1578e-04 - mean_absolute_error: 0.0104\n",
            "Epoch 104: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 2.1786e-04 - mean_absolute_error: 0.0104 - val_loss: 1.5039e-04 - val_mean_absolute_error: 0.0084\n",
            "Epoch 105/500\n",
            "133/133 [==============================] - ETA: 0s - loss: 1.9137e-04 - mean_absolute_error: 0.0100\n",
            "Epoch 105: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 1.9137e-04 - mean_absolute_error: 0.0100 - val_loss: 1.5988e-04 - val_mean_absolute_error: 0.0062\n",
            "Epoch 106/500\n",
            "130/133 [============================>.] - ETA: 0s - loss: 2.2610e-04 - mean_absolute_error: 0.0106\n",
            "Epoch 106: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 2.2790e-04 - mean_absolute_error: 0.0106 - val_loss: 1.4170e-04 - val_mean_absolute_error: 0.0066\n",
            "Epoch 107/500\n",
            "129/133 [============================>.] - ETA: 0s - loss: 2.3768e-04 - mean_absolute_error: 0.0105\n",
            "Epoch 107: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 2s 14ms/step - loss: 2.3567e-04 - mean_absolute_error: 0.0105 - val_loss: 1.5039e-04 - val_mean_absolute_error: 0.0070\n",
            "Epoch 108/500\n",
            "133/133 [==============================] - ETA: 0s - loss: 2.4161e-04 - mean_absolute_error: 0.0107\n",
            "Epoch 108: val_loss improved from 0.00012 to 0.00012, saving model to results/2023-03-10_AAPL-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
            "133/133 [==============================] - 2s 15ms/step - loss: 2.4161e-04 - mean_absolute_error: 0.0107 - val_loss: 1.2031e-04 - val_mean_absolute_error: 0.0054\n",
            "Epoch 109/500\n",
            "130/133 [============================>.] - ETA: 0s - loss: 2.0650e-04 - mean_absolute_error: 0.0102\n",
            "Epoch 109: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 2s 15ms/step - loss: 2.0633e-04 - mean_absolute_error: 0.0102 - val_loss: 2.2438e-04 - val_mean_absolute_error: 0.0102\n",
            "Epoch 110/500\n",
            "129/133 [============================>.] - ETA: 0s - loss: 2.2024e-04 - mean_absolute_error: 0.0105\n",
            "Epoch 110: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 2.2364e-04 - mean_absolute_error: 0.0106 - val_loss: 1.2389e-04 - val_mean_absolute_error: 0.0055\n",
            "Epoch 111/500\n",
            "130/133 [============================>.] - ETA: 0s - loss: 2.1771e-04 - mean_absolute_error: 0.0104\n",
            "Epoch 111: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 2s 14ms/step - loss: 2.1915e-04 - mean_absolute_error: 0.0105 - val_loss: 1.3214e-04 - val_mean_absolute_error: 0.0065\n",
            "Epoch 112/500\n",
            "131/133 [============================>.] - ETA: 0s - loss: 2.1794e-04 - mean_absolute_error: 0.0103\n",
            "Epoch 112: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 2.1723e-04 - mean_absolute_error: 0.0103 - val_loss: 1.2254e-04 - val_mean_absolute_error: 0.0064\n",
            "Epoch 113/500\n",
            "129/133 [============================>.] - ETA: 0s - loss: 2.3648e-04 - mean_absolute_error: 0.0106\n",
            "Epoch 113: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 2.3838e-04 - mean_absolute_error: 0.0106 - val_loss: 1.2175e-04 - val_mean_absolute_error: 0.0054\n",
            "Epoch 114/500\n",
            "129/133 [============================>.] - ETA: 0s - loss: 2.0858e-04 - mean_absolute_error: 0.0105\n",
            "Epoch 114: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 2s 14ms/step - loss: 2.1283e-04 - mean_absolute_error: 0.0106 - val_loss: 1.3270e-04 - val_mean_absolute_error: 0.0062\n",
            "Epoch 115/500\n",
            "130/133 [============================>.] - ETA: 0s - loss: 2.2900e-04 - mean_absolute_error: 0.0105\n",
            "Epoch 115: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 2s 14ms/step - loss: 2.2592e-04 - mean_absolute_error: 0.0105 - val_loss: 2.0801e-04 - val_mean_absolute_error: 0.0077\n",
            "Epoch 116/500\n",
            "131/133 [============================>.] - ETA: 0s - loss: 2.1658e-04 - mean_absolute_error: 0.0105\n",
            "Epoch 116: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 2s 15ms/step - loss: 2.1581e-04 - mean_absolute_error: 0.0105 - val_loss: 1.8979e-04 - val_mean_absolute_error: 0.0066\n",
            "Epoch 117/500\n",
            "129/133 [============================>.] - ETA: 0s - loss: 2.1213e-04 - mean_absolute_error: 0.0103\n",
            "Epoch 117: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 2s 14ms/step - loss: 2.1200e-04 - mean_absolute_error: 0.0103 - val_loss: 1.2471e-04 - val_mean_absolute_error: 0.0055\n",
            "Epoch 118/500\n",
            "130/133 [============================>.] - ETA: 0s - loss: 2.0643e-04 - mean_absolute_error: 0.0100\n",
            "Epoch 118: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 2s 14ms/step - loss: 2.0762e-04 - mean_absolute_error: 0.0100 - val_loss: 1.2609e-04 - val_mean_absolute_error: 0.0063\n",
            "Epoch 119/500\n",
            "132/133 [============================>.] - ETA: 0s - loss: 2.1257e-04 - mean_absolute_error: 0.0101\n",
            "Epoch 119: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 2.1213e-04 - mean_absolute_error: 0.0101 - val_loss: 1.2916e-04 - val_mean_absolute_error: 0.0061\n",
            "Epoch 120/500\n",
            "130/133 [============================>.] - ETA: 0s - loss: 2.1414e-04 - mean_absolute_error: 0.0101\n",
            "Epoch 120: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 2.1276e-04 - mean_absolute_error: 0.0101 - val_loss: 1.7938e-04 - val_mean_absolute_error: 0.0068\n",
            "Epoch 121/500\n",
            "129/133 [============================>.] - ETA: 0s - loss: 2.2400e-04 - mean_absolute_error: 0.0102\n",
            "Epoch 121: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 2s 14ms/step - loss: 2.2399e-04 - mean_absolute_error: 0.0103 - val_loss: 1.9269e-04 - val_mean_absolute_error: 0.0093\n",
            "Epoch 122/500\n",
            "133/133 [==============================] - ETA: 0s - loss: 2.2175e-04 - mean_absolute_error: 0.0105\n",
            "Epoch 122: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 2s 15ms/step - loss: 2.2175e-04 - mean_absolute_error: 0.0105 - val_loss: 1.3086e-04 - val_mean_absolute_error: 0.0060\n",
            "Epoch 123/500\n",
            "130/133 [============================>.] - ETA: 0s - loss: 2.1433e-04 - mean_absolute_error: 0.0101\n",
            "Epoch 123: val_loss improved from 0.00012 to 0.00012, saving model to results/2023-03-10_AAPL-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
            "133/133 [==============================] - 2s 14ms/step - loss: 2.1386e-04 - mean_absolute_error: 0.0101 - val_loss: 1.1914e-04 - val_mean_absolute_error: 0.0054\n",
            "Epoch 124/500\n",
            "131/133 [============================>.] - ETA: 0s - loss: 2.1535e-04 - mean_absolute_error: 0.0103\n",
            "Epoch 124: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 2.1695e-04 - mean_absolute_error: 0.0104 - val_loss: 1.2521e-04 - val_mean_absolute_error: 0.0070\n",
            "Epoch 125/500\n",
            "132/133 [============================>.] - ETA: 0s - loss: 2.1029e-04 - mean_absolute_error: 0.0101\n",
            "Epoch 125: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 2s 14ms/step - loss: 2.1008e-04 - mean_absolute_error: 0.0101 - val_loss: 1.5272e-04 - val_mean_absolute_error: 0.0076\n",
            "Epoch 126/500\n",
            "133/133 [==============================] - ETA: 0s - loss: 2.3303e-04 - mean_absolute_error: 0.0106\n",
            "Epoch 126: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 2s 14ms/step - loss: 2.3303e-04 - mean_absolute_error: 0.0106 - val_loss: 1.2296e-04 - val_mean_absolute_error: 0.0057\n",
            "Epoch 127/500\n",
            "131/133 [============================>.] - ETA: 0s - loss: 2.0821e-04 - mean_absolute_error: 0.0101\n",
            "Epoch 127: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 2s 14ms/step - loss: 2.0766e-04 - mean_absolute_error: 0.0101 - val_loss: 1.2132e-04 - val_mean_absolute_error: 0.0057\n",
            "Epoch 128/500\n",
            "132/133 [============================>.] - ETA: 0s - loss: 1.9891e-04 - mean_absolute_error: 0.0100\n",
            "Epoch 128: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 2s 14ms/step - loss: 1.9871e-04 - mean_absolute_error: 0.0100 - val_loss: 1.2384e-04 - val_mean_absolute_error: 0.0060\n",
            "Epoch 129/500\n",
            "129/133 [============================>.] - ETA: 0s - loss: 2.2034e-04 - mean_absolute_error: 0.0104\n",
            "Epoch 129: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 2s 15ms/step - loss: 2.1765e-04 - mean_absolute_error: 0.0104 - val_loss: 1.3010e-04 - val_mean_absolute_error: 0.0077\n",
            "Epoch 130/500\n",
            "129/133 [============================>.] - ETA: 0s - loss: 2.2049e-04 - mean_absolute_error: 0.0103\n",
            "Epoch 130: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 2.2097e-04 - mean_absolute_error: 0.0103 - val_loss: 1.3620e-04 - val_mean_absolute_error: 0.0055\n",
            "Epoch 131/500\n",
            "133/133 [==============================] - ETA: 0s - loss: 2.1154e-04 - mean_absolute_error: 0.0101\n",
            "Epoch 131: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 2s 14ms/step - loss: 2.1154e-04 - mean_absolute_error: 0.0101 - val_loss: 1.2101e-04 - val_mean_absolute_error: 0.0054\n",
            "Epoch 132/500\n",
            "133/133 [==============================] - ETA: 0s - loss: 2.1550e-04 - mean_absolute_error: 0.0101\n",
            "Epoch 132: val_loss did not improve from 0.00012\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 2.1550e-04 - mean_absolute_error: 0.0101 - val_loss: 1.2516e-04 - val_mean_absolute_error: 0.0059\n",
            "Epoch 133/500\n",
            " 64/133 [=============>................] - ETA: 0s - loss: 1.9762e-04 - mean_absolute_error: 0.0098"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-0772416524a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# train the model and save the weights whenever we see\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# a new optimal model using ModelCheckpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m history = model.fit(data[\"X_train\"], data[\"y_train\"],\n\u001b[0m\u001b[1;32m     15\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1648\u001b[0m                         ):\n\u001b[1;32m   1649\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1650\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1651\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    910\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m       (concrete_function,\n\u001b[1;32m    133\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 134\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    135\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1743\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1745\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    376\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    379\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "data = load_data(ticker, N_STEPS, scale=SCALE, split_by_date=SPLIT_BY_DATE, \n",
        "                shuffle=SHUFFLE, lookup_step=LOOKUP_STEP, test_size=TEST_SIZE, \n",
        "                feature_columns=FEATURE_COLUMNS)\n",
        "# save the dataframe\n",
        "data[\"df\"].to_csv(ticker_data_filename)\n",
        "# construct the model\n",
        "model = create_model(N_STEPS, len(FEATURE_COLUMNS), loss=LOSS, units=UNITS, cell=CELL, n_layers=N_LAYERS,\n",
        "                    dropout=DROPOUT, optimizer=OPTIMIZER, bidirectional=BIDIRECTIONAL)\n",
        "# some tensorflow callbacks\n",
        "checkpointer = ModelCheckpoint(os.path.join(\"results\", model_name + \".h5\"), save_weights_only=True, save_best_only=True, verbose=1)\n",
        "tensorboard = TensorBoard(log_dir=os.path.join(\"logs\", model_name))\n",
        "# train the model and save the weights whenever we see \n",
        "# a new optimal model using ModelCheckpoint\n",
        "history = model.fit(data[\"X_train\"], data[\"y_train\"],\n",
        "                    batch_size=BATCH_SIZE,\n",
        "                    epochs=EPOCHS,\n",
        "                    validation_data=(data[\"X_test\"], data[\"y_test\"]),\n",
        "                    callbacks=[checkpointer, tensorboard],\n",
        "                    verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xval = EPOCHS\n",
        "yval = validation_data\n",
        "plt.plot(xval,yval)\n",
        "plt.show"
      ],
      "metadata": {
        "id": "-GsTQ2Xg38MY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}